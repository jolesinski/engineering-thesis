\chapter{Implementation and testing}
\label{cha:implandtest}

A MMS robot equipped with a depth acquisition system is a particularly well suited tool to perform tasks with a high degree of autonomy. The high volume of environmental information provided by a depth sensor allows the robot to successfully operate in an unstructured and changing environment without human guidance. In this work, the task which the robot has to solve fully autonomously is the problem of finding a predefined object in its working environment. This problem required implementing two mechanisms for depth data processing. The first was the detection of obstacles, in order to ensure save movement of the mobile platform and the second one was the recognition of the object to accomplish the final task. The Point Cloud Library (PCL) \cite{Rusu_ICRA2011_PCL} was used to implement the depth data analysis on the robot. PCL is an open source project for 3D point cloud processing. The library contains a vast array of state-of-the-art algorithms for filtering, feature estimation, segmentation, registration and model fitting. OpenMP and FLANN. A detailed description of the developed autonomous control mode is provided in the following sections.

%---------------------------------------------------------------------------

\section{Hardware setting}
\label{sec:setting}

The first issue encountered during the development of the autonomous mode was adequate positioning of the depth camera. As mentioned in section \ref{sec:3dsum}, the selected RGB-D sensor, Asus Xtion Pro operating range starts from $0.8m$, which is a relatively high distance compared to the dimensions of the whole robot. For this reason, the camera position and field of view determines the size of objects that can be used as targets. 	Moreover, the RGB-D camera location is also crucial during obstacle detection. The system should be able to determine whether it is possible to displace an entire width of the platform forward. While moving ahead, the analysed area is desired to be large enough that the robot could react to obstacle on time and avoid collision. On the other hand, the region close to the front of the platform should also be examined, as obstacles are likely to occur there during turning.

The mounting point of the RGB-D camera has been selected in place of an end-effector of the larger manipulator. This setting has allowed for reproducible and convenient adjustments of the camera pose. The Figure \ref{fig:fov} presents a simplified view of camera setting relative to the mobile platform. The manipulator is positioned in the plane compliant with the drawing and only the joints relevant to this plane are specified. The joint angles $\theta_1, \theta_2, \theta_3$ were experimentally selected to meet the previously mentioned requirements. The obtained values $\theta_1 = 22^o, \theta_2 = 22^o , \theta_3 = 22^o$,
together with physical dimensions $L_0 = 22^o, L_1 = 22^o, L_2 = 22^o, L_3 = 22^o$
allow to calculate the camera height $H$ as follows:
$$ H = L_0 + L_1 sin(180^o-\theta_1)+L_2 sin(\theta_1 - \theta_2) + L_3 sin(\theta_1 - \theta_2 - \theta_3) = x $$


\input{fig/fieldofview}

Problem of high distance. Why such view: on a manipulator - easily adjusted, for future developments, manipulator angles => how to calc camera angle, angle - to see max closely to the front of the platform, and to be able to see objects of about 20x20 at 0.5 m.

Image processing on the NVidia board? here or in the firs chap

%---------------------------------------------------------------------------

\section{Obstacle detection implementation}
\label{sec:objavoid}

Knowledge of the camera field of view facilitates the first stage of depth data processing, the obstacle detection. According to it, the view acquired from the sensor can be further adjusted to detect objects only in the designated area. The full, implemented processing pipeline, including this view adjustments, is presented in the Figure \ref{fig:detectpipe}.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
  [node distance = 5mm,auto,every node/.style={rectangle,draw,align=center, font=\footnotesize}]
  
  \node (down) at (1,10) {Downsampling};
  \node[right=of down] (plane) {Plane model \\ Extraction};
  \node[right=of plane] (trans) {Affine \\ Transformation};
  \node[right=of trans] (pass) {Passthrough \\ Filtering};
  \node[right=of pass] (outlier) {Outlier \\ Removal};

  \foreach \from/\to in {down/plane, plane/trans, trans/pass, pass/outlier}
    \draw[->] (\from) -- (\to);

\end{tikzpicture}

\caption{Implemented obstacle detection pipeline}

\label{fig:detectpipe}

\end{center}
\end{figure}

Depth sensors provide a high volume of information, which has to be analysed online with a limited computing resources. Therefore, the first processing step was to reduce the amount of data by downsampling. For this purpose, a voxel filter with a leaf size of $1cm \times 1cm \times 1cm$ has been applied. Figure x illustrates an exemplary point cloud before and after filtration. The number of points has been reduced from $x$ to $y$. This step has significantly accelerated further processing stages, thus allowing a sufficiently fast response to obstacle emergence.

FIGURE

By design, the robot is intended to work in the indoor environment. A significant feature of such environment is the planar surface of the floor. Therefore, by fitting a plane model to the acquired point cloud, it is possible to determine the  area belonging to the ground, and thus free of obstacles. The RANSAC algorithm implementation, available in the PCL library, was used to estimate the plane model parameters $A,B,C,D$, given by Equation \ref{eq:planemodel}. Dobrane parametry? After the fitting, all of the model inliers are removed from the point cloud. This step thus leaves only the points that potentially belong to an obstacle. On the other hand, if the RANSAC algorithm fails to fit the model, it is assumed that the entire view is occluded by obstacles and the procedure returns positive detection.  The Figure x illustrates the point cloud from Figure x after plane model extraction.

FIGURE

The next processing step, the volume of interest (VoI) extraction, involves separation of a cuboidal space from the remaining point cloud. The dimensions of the extracted cuboid directly correspond to the total height and width of the robot, and the maximum distance at which the system is supposed to react to obstacles. As shown in Figure \ref{fig:fov}, the camera is pointing to the floor at a certain angle. This direction is represented by a Z-axis of the coordinate system in the acquired point cloud. To enable the VoI extraction with the specified dimensions, the coordinate system of the cloud is rotated by an angle $\theta$ around the X-axis, such that the Z-axis of the resultant coordinate frame is parallel to the floor. This transformation can be based on the angle $jakistam$, calculated in Section \ref{sec:setting}. However, because of the vibrations caused by the platform movement, a more accurate solution is to use the previously calculated plane model. A vector $\vec{n}$ normal to a plane \ref{eq:planemodel} is given by:
\begin{equation}
 \vec{n} = \frac{1}{\sqrt{A^2+B^2+C^2}} \cdot \left[
\begin{array}{c}
A\\
B\\
C\\
\end{array}
\right] 
\end{equation}
It can be shown, that the third coordinate $n_z$ of the normal vector $\vec{n}$ satisfies $n_z = sin(\theta)$ and the final rotation matrix is given by:

\begin{equation}
R_x=\left[\begin{array}{ccc}
      1 & 0 & 0  \\
      0 & \sqrt{1 - n_z^2} & -n_z  \\
      0 & n_z & \sqrt{1 - n_z^2}  \\
    \end{array}\right]
\end{equation}
After viewpoint transformation, the VoI is extracted by applying a Passthrough filter along X, Y and Z axes of the received cloud. The filtration ranges have been experimentally selected...

The last stage of the obstacle detection processing is the outlier removal.  
	

%---------------------------------------------------------------------------

\section{Object recognition implementation}
\label{sec:detection}

In addition to ensuring safe movement of the robot, previously described obstacle  detection pipeline also performs most of the necessary preprocessing steps for object recognition. Upon obstacle detection, the resulting point cloud contains points that belong only to the objects in the scene, thus further processing is directly applied to that cloud. 

Two different approaches of object recognition were implemented during the project. The first one uses an analytical model of the objects shape and allows to recognize objects with simple geometric structures, such as spheres or cylinders. The second, more general method is based on the objects detailed model, acquired during 3D scanning. Despite the differences, both of these approaches share some common preprocessing steps, including the normal estimation and cluster segmentation. To determine the normal vectors for a given cloud, the NormalEstimationOMP class, available in PCL library, is used. This class exploits the OpenMP [x] library to parallelize computations, which significantly accelerates the process of normal estimation. 

	It is assumed that during the exploration of the environment, the robot can encounter a scene with multiple objects. In such situation, it is convenient to isolate individual items and analyse them separately. For this purpose, a greedy algorithm of Euclidean Cluster Extraction is applied to the point cloud. The parameters of the algorithm were experimentally chosen, to match the size of individual clusters with objects used as targets. VALUES. The last parameter, due to the nature of the euclidean segmentation, imposes a minimal distance of $10cm$ between object surfaces. As a result of the segmentation process, a vector of point clouds is obtained and further analysis is applied to each of its elements.
	
	In case of the first, geometric primitives based approach, the model fitting is performed directly on the point clouds obtained after the segmentation. The SACSegmentationFromNormals class from PCL library is used during the fitting process. In this approach, the recognition of two geometric shapes is implemented, the sphere and the cylinder. As a result of fitting a sphere, the algorithm returns four optimized parameters: three coordinates of the sphere center point and its radius. An example of spherical object fitting is presented in the Figure X.
	
FIGURE

For the cylinder model, seven parameters are obtained, including the radius, three coordinates of its axis direction and three coordinates of a point lying on that axis. Furthermore, in order to distinguish cylindrical objects with different heights a few additional steps are required after the model is found. Firstly, the point cloud coordinate system is transformed so as to cover the cylinder axis with the Z-axis. Next, two points with minimum and maximum $z$-coordinates are found with the getMinMax3D function from PCL. Finally, the resulting height $h$ of the cylinder is given by the difference of those coordinates, that is $h = z_{max} - z_{min}$. Figure X illustrates an example input cloud with a fitted cylinder model.

FIGURE

The second, general method of object recognition utilises the global recognition pipeline described in Section \ref{sec:pipeline}. This selection is justified by the fact, that the global approach requires a set of partial object scans as a training set, which can be easily acquired with available hardware. The VFH descriptor, estimated by the PCL VFHEstimation class, is used to compare the point clusters. To obtain the training data set, a series of scans by the RGB-D camera are taken from different, characteristic views of the object. For each of these scans, the VFH descriptor is calculated and added to a KD-Tree structure, which is then saved to the hard drive. During the recognition process, for a given VFH descriptor of a cluster, a nearest-neighbour search is performed in the KD-Tree structure, by the \textit{knnSearch} method from the FLANN library. If the resultant $L_1$ distance between the neighbour and the segment VFH is less than a given threshold, the procedure returns positive object recognition. The further post-processing steps, listed in Section \ref{sec:pipeline}, which estimate the 6DOF pose of an object were omitted, as the task of the robot is to only determine the existence of a given object.

%---------------------------------------------------------------------------

\section{Autonomous mode implementation}
\label{sec:autonomy}

Procedures of obstacle detection and object recognition provide the necessary tools to achieve the stated task of autonomous finding of predefined objects. The autonomous mode was implemented in the form of a C++ application, executed on the NVIDIA Jetson TK1 platform. This application communicates with the control server via HTTP requests, as described in Section \ref{sec:soft}. For this purpose, the HTTP POST and GET methods from cpp-netlib [??] library are utilised. All of the required communication request are encapsulated in the form of a Robot class object. The methods of this class enable the realization of such actions as driving forward, mobile platform rotation, stopping and setting the manipulator joints in given positions. 

The depth data processing procedures described in preceding sections posses a significant amount of configurable parameters. To avoid having to recompile the code with every parameter customization, all of the relevant constants are included in a configuration file in a XML format. The path to this XML file is specified as a command-line argument of the application, together with the name of  the target to search for. The configuration file contains also information on the recognition method type and the path to the training set for the specified target name.

\tikzset{
    state/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           minimum height=2em,
           inner sep=2pt,
           text centered,
           },
}

\begin{figure}[H]
\begin{centering}

\begin{tikzpicture}[->,>=stealth']

 % Position of MOVE 
 % Use previously defined 'state' as layout (see above)
 % use tabular for content to get columns/rows
 % parbox to limit width of the listing
 \node[state,
  text width=3cm] (MOVE) 
 {\begin{tabular}{l}
  \textbf{MOVE}
 \end{tabular}};
  
 
 % STATE QUERYREP
 \node[state,
  below right of=MOVE,
  node distance=3cm, 	% distance to QUERY
  %yshift=-2cm,
  anchor=center,
  text width=3cm] (STOP) 
 {%
 \begin{tabular}{l}
  \textbf{STOP}
 \end{tabular}
 };
 
 % State: TURN with different content
 \node[state,    	% layout (defined above)
  text width=3cm, 	% max text width
  %yshift=2cm, 		% move 2cm in y
  above right of=STOP, 	% Position is to the right of QUERY
  node distance=3cm, 	% distance to QUERY
  anchor=center] (TURN) 	% posistion relative to the center of the 'box'
 {%
 \begin{tabular}{l} 	% content
  \textbf{TURN}
 \end{tabular}
 };

 % draw the paths and and print some Text below/above the graph
 \path (MOVE.north) 	edge[bend left=20]  node[anchor=south,above]{$SC_n=0$}
                                    node[anchor=north,below]{$RN_{16}$} (TURN.north)
 (MOVE)     	edge[bend right=20] node[anchor=south,above]{$SC_n\neq 0$} (STOP.west)
 (STOP)  	edge[loop below] node[anchor=north,below]{$SC_n\neq 0$} (STOP)
 (STOP.east)  	edge[bend right=20] node[anchor=left,right]{$SC_n = 0$} (TURN);

\end{tikzpicture}

\caption{State diagram of the autonomous mode}
\label{fig:statetrans}

\end{centering}
\end{figure}

The system operation outline is presented in the Figure \ref{fig:statetrans}, in the form of a state diagram. Three different states of motion are distinguished: forward movement (\textbf{F}), turning (\textbf{T}) and stopping (\textbf{S}). 
At the time of entrance to each of the \textbf{F}, \textbf{T}, \textbf{S} states, a corresponding command is sent to the control server. Transitions between these states are determined by the logical values returned from the procedures of obstacle detection (\textit{OD}) and object recognition (\textit{OR}). The robot moves forward if there are no obstacles in front of it, stops if it encounters the target object and turns if it discovers an obstacle. The direction of turning is selected in a pseudo-random manner at each newly encountered obstacle. In this way, the system is able to explore the whole working environment. 

%---------------------------------------------------------------------------

\section{Testing environment and results}
\label{sec:testing}

